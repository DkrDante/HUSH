# HUSH ðŸ¤š

**Real-time Indian Sign Language (ISL) Alphabet Recognition**

Built with MediaPipe â€¢ FastAPI â€¢ WebSockets â€¢ Vanilla JS

---

## Quick Start

```bash
# Clone / navigate to the project
cd /path/to/hush

# Run everything in one command
bash run.sh
```

Then open **<http://localhost:8000>** in your browser.

---

## Project Structure

```
hush/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py               # FastAPI app + WebSocket endpoint
â”‚   â”œâ”€â”€ gesture_classifier.py # MediaPipe hand landmark classifier
â”‚   â””â”€â”€ isl_gestures.py       # ISL Aâ€“Z gesture data
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html            # Landing page
â”‚   â”œâ”€â”€ app.html              # Main recognition app
â”‚   â”œâ”€â”€ reference.html        # ISL alphabet guide
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ style.css         # Design system (tokens, animations)
â”‚   â”‚   â”œâ”€â”€ index.css         # Landing page styles
â”‚   â”‚   â”œâ”€â”€ app.css           # App page styles
â”‚   â”‚   â””â”€â”€ reference.css     # Reference page styles
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ app.js            # WebSocket + webcam + UI logic
â”‚       â”œâ”€â”€ reference.js      # Alphabet grid + modal
â”‚       â””â”€â”€ index.js          # Landing page demo animation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh                    # One-command startup
â””â”€â”€ README.md
```

---

## API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/` | Landing page |
| GET | `/app` | Recognition app |
| GET | `/reference` | ISL alphabet guide |
| GET | `/api/health` | Health check + uptime |
| GET | `/api/gestures` | All ISL letters (Aâ€“Z) with descriptions |
| GET | `/api/gestures/{letter}` | Single letter detail |
| GET | `/api/stats` | Session frame/detection statistics |
| POST | `/api/stats/reset` | Reset session stats |
| WS | `/ws` | Real-time frame â†’ gesture classification |

---

## WebSocket Protocol

**Send** (client â†’ server): Base64-encoded JPEG frame string

**Receive** (server â†’ client):

```json
{
  "type": "result",
  "hand_detected": true,
  "letter": "A",
  "pending_letter": "A",
  "confidence": 0.82,
  "stable": true,
  "landmarks": [{"x": 0.5, "y": 0.3, "z": -0.02}, ...]
}
```

---

## How It Works

1. **Webcam** frames are captured every ~150ms via `getUserMedia`
2. Frames are JPEG-compressed and sent as base64 over **WebSocket**
3. **MediaPipe Hands** extracts 21 3D hand landmarks server-side
4. A **rule-based classifier** maps landmark geometry â†’ ISL letter
5. A **stability filter** (3 consistent frames) prevents flickering
6. Result is sent back to the browser and displayed in real-time

---

## Gesture Coverage (ISL Aâ€“Z)

All 26 letters are supported using one-hand static gestures.
Dynamic letters (J, Z) are approximated by their starting position.

---

## Accessibility

- ARIA live regions announce detected letters to screen readers
- Full keyboard navigation (Tab, Enter, Space, Backspace)
- High-contrast mode toggle
- WCAG AA color contrast ratios throughout

---

## Requirements

- Python 3.9+
- Webcam
- Modern browser (Chrome, Firefox, Safari)
- No GPU required (CPU inference via MediaPipe)
# HUSH
